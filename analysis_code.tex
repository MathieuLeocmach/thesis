\chapter{Analysis methods}

\section{Fast spatial query}
\label{sec:spatial_query}

To analyse efficiently structures in real space one often needs to find all the objects (particles) that belong to a given portion of space. For instance, given a particle $p$ we may want to know all the particles $\{q\}$ that are closer than a distance $r$ from $p$. A brute force computation would implies the calculation of $N$ distances, with $N$ the total number of particles in the system. Because we have to know the neighbours of each particles, the construction of the total bond network cost $O(N^2)$ operations. Thus, a typical configuration of $5 \cdot 10^4$ particles would take about \unit{10}{\minute} to be processed, leading to more than half a day of computation for $200$ time steps. This is unreasonable.

The \emph{range search problem}, also known as fixed-radius near-neighbours search~\citep{Dickerson1996}, is a very important computational geometry problem with numerous applications across a large range of disciplines, including geographical information systems, computer graphics, astrophysics, pattern recognition, databases, data mining, artificial intelligence and bioinformatics~\citep{Chavez2001}. Other variants of this problem include nearest neighbour query, k-nearest neighbour query, spatial join, and approximate nearest neighbour~\citep{Roussopoulos1995, Chavez2001}. This class of problem is called \emph{spatial query} and its fast resolution is crucial to now everyday-use applications like car navigator or web mapping services. We will use here such prosaic examples; for concrete applications in the field of this thesis see \SectionRef{sec:time_tracking} and \SectionRef{sec:neighbours_bonds}.

\subsection{Notion of index}

Imagine that we are looking in advance for a restaurant near the station of a city we are going to visit for the first time. The twentieth century way of doing that is to take the business phone book at the restaurant chapter and look up one by one the addresses to select the restaurants that are close to the station. The time taken by this process is proportional to the length $N$ of the list: we need to perform $N$ lookup operations. Hopefully, we have the phone book of this city, subdivided in districts, not the alphabetical list of all the restaurant of the country: the smaller is $N$, the better.

The phone book is organised so that the list we finally have to look up in is the shortest possible: we start by selecting on the shelf the book corresponding to the prefecture, then we go to the part dedicated to the city and then to the chapter listing restaurants. This type of hierarchical structure is called an index. Looking up using an index is a very fast operation, typically one needs only $O(\log C)$ operations to find the right chapter, with $C$ the number of chapters. It is so because an index (or lookup table) is sorted by a useful criterion, like the names of the towns in the prefecture. Finding an element of a sorted list is equivalent to finding a word in the dictionary: we can perform a search by dichotomy without reading all the words of the dictionary.

The drawback of lookup tables is their linearity. Crossing two criteria like the location and the type of business is possible with a hierarchy of indices. However, such a hierarchy often proves counter-productive. If our goal is to find a Chinese restaurant, wherever it is located in the city, we have to look through the restaurants of each district to select the Chinese ones. The hierarchy of: location to the precision of the district, then restaurant, then type of restaurant is not always optimal, as neither of the other ways of organising these three criteria.

\subsection{Spatial index}

Most guidebooks have a map at the head of each chapter indicating the location of interesting places. An icon of fork and knife on the map indicates a restaurant, and its description is pointed by the number nearby. This is an other form of lookup, in two dimensions this time. The human reader instantaneously finds the list of all the restaurants within a certain range of the station. How would a computer do?

For the computer, the map of the restaurants is a table containing the coordinates $(x,y)$ of each restaurant $q$. Given the position $(x_0, y_0)$ of the station and the range $r_0$, one may compute all the Euclidean distances $r = \sqrt{(x-x_0)^2 + (y-y_0)^2}$ and test them to select those with $r<r_0$. Once again this require $O(N)$ operations, acceptable if the number of restaurants is small, but incredibly slow if all the restaurants of the planet are referenced in the same map. That is obviously not the way Google Maps is proceeding.

A more clever way is to divide the space into a grid. That is how the guidebook proceeds for reverse lookup: at the head of the paragraph describing the station it is written $D5$, so one looks in the $D5$ square of the map to find the station. That implies that each time the author of the guidebook adds a new restaurant, he has to affect it to a square of the grid. In the same way, the computer divides space in a $\{X_0, X_1, \ldots, X_N\}\times\{Y_0, Y_1, \ldots, Y_M\}$ grid, with a list of restaurants for each $(X_i, X_{i+1}) \times (Y_j, Y_{j+1})$ rectangle. Once the grid is constructed, the computer can find in one operation the $i$ so that $X_i \leq x_0 < X_{i+1}$, and the same for $j$. This immediately gives the list of all the restaurants within the same rectangle as the station and we can then compute the euclidean distances within this short list. We may have to look also in the neighbouring rectangles with respect to $r_0$.

The construction of the grid took $O(N)$ operations and the lookup is done at most in $O(n s(r_0))$ with $n$ the maximum number of restaurants in the same rectangle of the grid and $s(r_0)$ the number of squares that were included in the range. This means that the grid is computationally efficient if we have to perform many lookups and if the step of the grid is narrow enough to make $n$ small, but not too narrow compared to $r_0$. These conditions are met if the query ranges are not too different from each other and if the coordinates uniformly cover space. Grids are not optimized for datasets with large voids like worldwide geographical data (restaurants are not as densely packed in the middle of the ocean as in a big city) or the particles of a gel. The grid of such datasets must have a step narrow enough to be useful in the dense areas, which produces many empty or nearly empty squares in the less dense area, consuming memory and lookup time.

\subsection{R-tree}

A grid is only one of the possible spatial indices, maybe the most simple and surely the most used in molecular simulations of dense phases. However, soft matter systems may require a more evolved data structure that can deal with many scales of organisation, for example the scale of the particles, the thickness of an aggregate of particles and the scale of the overall network formed by theses aggregates. Indexing such a structured space is reminiscent of the problems faced by the geographers: referencing buildings, cities and continents with the same tool. Geographers use various types of spatial indices, including the R-tree~\citep{Guttman1984}, kd-tree~\citep{Bentley1975} or quad-trees~\citep{Finkel1974}. We decided to use the R-tree and more specifically its variant called R*-tree~\citep{Beckmann1990}, however our code can easily accept any spatial index.

The R-Tree is a data structure where each particle is in a (zero-sized) box, itself nested in a box that contains $n$ boxes, itself nested in a box $\ldots$ itself nested in the box than contains all the system. At each level, the box fits exactly its contain. That is why they are called minimum bounding boxes (in 2D "rectangles", what the "R" in R-tree stands for). A 2D example is shown in \FigureRef{fig:rtree}.

\begin{figure}
	\centering
	\def\svgwidth{0.8\textwidth}\input{R-tree.pdf_tex}
	\caption{Simple example of an R-tree for 2D rectangles indexing the green dots. Each node can contain at least three children. Source: Wikipedia.}
	\label{fig:rtree}
\end{figure}

Each node of an R-tree has a variable number of entries (up to some pre-defined maximum). Each entry within a non-leaf node stores two pieces of data: a way of identifying a child node, and the bounding box of all entries within this child node.

The insertion and deletion algorithms use the bounding boxes from the nodes to ensure that "nearby" elements are placed in the same leaf node (in particular, a new element will go into the leaf node that requires the least enlargement in its bounding box). Each entry within a leaf node stores two pieces of information; a way of identifying the actual data element (which, alternatively, may be placed directly in the node), and the bounding box of the data element.

Similarly, the searching algorithms use the bounding boxes to decide whether or not to search inside a child node. In this way, most of the nodes in the tree are never "touched" during a search. A range search is performed in $O(N)$ operations.

Various implementations of the R*-tree are available open source (see ~\citep{Theodoridis2003} for a complete list). Among them, we gave a try to: 
\begin{itemize}
	\item \acs{RDMS} like SQLite, PostgreSQL or more recently MySQL include spatial indices based on R-trees. These implementations make sense only when one wants to retrieve data attached to shapes in space, not for geometrical computations.
	\item The well named Spatial Index Library~\citep{Hadjieleftheriou2005} that is for sure the best one available, especially for time series. However it is quite arcane for somebody outside computer science, and lacks support and a proper manual.
	\item The lightweight implementation by \citet{Spicuzza2008}. This is the one integrated into our code.
\end{itemize}

Provided the spatial indexing, we were able to perform the bond network construction in $O(N\log N)$ operations, \latin{i.e.} \unit{1\sim 2}{\second} for $5 \cdot 10^4$ particles, leading to only \unit{5\sim 10}{\minute} of computation for $200$ time steps.

\section{Analysis of an open system}

Contrary to most simulations, the data we have to analyse are not contained within periodic boundary conditions or a closed box. We observe only a fraction of the sample by confocal microscopy, and the boundary between the observed and the unobserved part has no physical meaning. This causes many issues during the analysis of the structure.

\begin{figure}
	\centering
	\def\svgwidth{0.8\textwidth}\input{margin.pdf_tex}
	\caption{Schematic view of our boundary conditions. The thick black line is the limit of the microscope field of view. The dotted particles are physically present but not detected because outside the field of view. The circles represent the maximum bond length (the range of the analysis). In orange, two bonds that physically exist but are not detected, leading to an artefactual anisotropy in the neighbourhood of the margin particles (cyan). The particles that can be correctly analysed (dark green) are further than $r_b$ from the limit of the field of view.}
	\label{fig:margin}
\end{figure}

To construct the bond network we select the pairs of particles $(i, j)$ so that $r_{ij}<r_b$, with $r_b$ the maximum bond length. Unfortunately (see \FigureRef{fig:margin}), the bonds between a particles inside the field of view and a particle outside the field of view (thus not detected) are not constructed. This means that the particles closer than $r_b$ to the boundary may have a non-physical lack of neighbours in the direction of the boundary. To avoid this problem, we analyse only the particles further than $r_b$ from any boundary. The particles at the margin are our boundary conditions. If the analysis implies a coarse-graining over the first shell, we have to remove a second margin of $r_b$.

The analysis of long range correlations is more of a problem. The bulk expression of the \ac{RDF} (\EquationRef{eq:rdf_bulk}) stands if we conduct it on a reduction for our experimental window. This limits us to a theoretical maximum range $r_{max}=L/2$ with $L$ the smaller dimension of our experimental window. When reaching this limit, there is no particle further enough from the boundaries. A more reasonable maximum range is $L/4$, a compromize between the number of particles left for the analysis and the maximum range of the correlation. 

To exploit the full size of our box, we have to go back to the definition of the \ac{RDF}
\begin{equation}
	g(r) \equiv \frac{1}{N} \sum_{i}^N \frac{
		\sum_{j\neq i} \delta(r_{ij} - r)
		}{
		\int_V \rho \delta(r^\prime-r) dV
		}
	\tag{\ref{eq:rdf}}
\end{equation}

In our case, the numerator is not a simple expression but can be simulated by filling our experimental windows with random coordinates (an ideal gas) and compute actually the ratio between the probability to find a particle at a distance $r$ in the real system and the same probability in the ideal gas. If not done on enough random coordinates, this procedure can add a non-negligible amount of noise to the data.

When correlating an other observable $a(\vec{r})$, like the tensorial bond order parameter (see \SectionRef{sec:boo_correl} and \SectionRef{sec:mrco_spatial}), the radial correlation of the observable is defined as:

\begin{equation}
	\mathcal{C}(a)(r) \equiv \frac{1}{N}\sum_i^N \frac{
		\sum_{j\neq i}{ a(\vec{r}_i) \cdot a^*(\vec{r}_j) \delta\left(\left\|\vec{r}_i-\vec{r}_j \right\| - r \right)}
	}{
	\sum_{j \neq i}{\delta\left(\left\|\vec{r}_i-\vec{r}_j \right\| - r \right)}
	}
	\label{eq:obervable_correlation}
\end{equation}

We recognise in the denominator of \EquationRef{eq:obervable_correlation} the numerator of the \ac{RDF} (\EquationRef{eq:rdf}) that can be calculated everywhere $a(\vec{r})$ is defined. A common mistake would be to compute independently the ensemble averages of the numerator and denominator of \EquationRef{eq:obervable_correlation} and then divide one by the other. This error is silent in periodic boundary conditions but not in our open system.
